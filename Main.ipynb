{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45811,
     "status": "ok",
     "timestamp": 1638645394108,
     "user": {
      "displayName": "Jamshid Mozafari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLKdl-M6ZkOmHKMDfhTduwgXYNb7xMJFVQ5D12=s64",
      "userId": "09439038486094053669"
     },
     "user_tz": -210
    },
    "id": "sv5VcJffOOKX",
    "outputId": "55086cec-53aa-46e7-cd21-3bbd5cf0ff3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU:  Tesla K80\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install -q sentencepiece\n",
    "\n",
    "import sys\n",
    "import os.path as path\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sys.path.append(path.join('drive','My Drive','Colab Notebooks'))\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "clear_output()\n",
    "if device_name == '/device:GPU:0':\n",
    "  print('GPU: ',torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 217604,
     "status": "ok",
     "timestamp": 1638645611679,
     "user": {
      "displayName": "Jamshid Mozafari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLKdl-M6ZkOmHKMDfhTduwgXYNb7xMJFVQ5D12=s64",
      "userId": "09439038486094053669"
     },
     "user_tz": -210
    },
    "id": "06nwMWNVFeDF",
    "outputId": "95bcac7a-f548-450b-c161-82a250a7bc79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1638645611680,
     "user": {
      "displayName": "Jamshid Mozafari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLKdl-M6ZkOmHKMDfhTduwgXYNb7xMJFVQ5D12=s64",
      "userId": "09439038486094053669"
     },
     "user_tz": -210
    },
    "id": "wXTbMtEtOPD2",
    "outputId": "d8bab535-9af6-4137-c634-bd00002e0413"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd '/content/drive/My Drive/Colab Notebooks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257,
     "referenced_widgets": [
      "eeadf78980f54a858ace61f6dd666dc7",
      "965b985e83d949f08abc626ce0aa395b",
      "a6e77a2835434c29af141cae971791d9",
      "c1e162c9fb3f4d4289d274ef0d31cfec",
      "69f8bfff9a2c443194c1a8e681e63e81",
      "e56321fea37d45f6a028278d64ed2943",
      "10b4616710d04a9dab4a084decd9eb61",
      "7534515187104b7a861b67f18119768a",
      "2976524682494178a178f89d6b8d74e2",
      "fa8754f4faf94163ad1a72f237d1aa98",
      "9afba3641a884e71bcf5fe7f422f7f83"
     ]
    },
    "executionInfo": {
     "elapsed": 146055,
     "status": "ok",
     "timestamp": 1638645757726,
     "user": {
      "displayName": "Jamshid Mozafari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLKdl-M6ZkOmHKMDfhTduwgXYNb7xMJFVQ5D12=s64",
      "userId": "09439038486094053669"
     },
     "user_tz": -210
    },
    "id": "MoNgozonOQ2g",
    "outputId": "0ce01471-f9f0-438e-e682-12bffe03855a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate the last checkpoint\n",
      "Loading features from cached file dataset/cached_dev\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeadf78980f54a858ace61f6dd666dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluation done in total 109.648759961 secs (0.05092836040919647 sec per example)\n",
      "{'exact': 78.7, 'f1': 82.97251044277358, 'total': 1000, 'HasAns_exact': 78.7, 'HasAns_f1': 82.97251044277358, 'HasAns_total': 1000, 'best_exact': 78.7, 'best_exact_thresh': 0.0, 'best_f1': 82.97251044277358, 'best_f1_thresh': 0.0}\n",
      "\n",
      "Result:\n",
      "\texact_match: 78.7\n",
      "\tf1: 82.97251044277358\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import timeit\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    squad_convert_examples_to_features,\n",
    ")\n",
    "from transformers.data.metrics.squad_metrics import (\n",
    "    compute_predictions_logits,\n",
    "    squad_evaluate,\n",
    ")\n",
    "from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "\"\"\"Parameters\"\"\"\n",
    "\n",
    "#model_name_or_path = 'bert-base-cased'\n",
    "#model_name_or_path = 'm3hrdadfi/albert-fa-base-v2'\n",
    "#model_name_or_path = 'HooshvareLab/bert-fa-base-uncased'\n",
    "model_name_or_path = 'bert-base-multilingual-cased'\n",
    "output_dir = 'model'\n",
    "dataset_name = ''\n",
    "version_2 = False\n",
    "null_score_diff_threshold = 0\n",
    "max_seq_length = 384\n",
    "max_query_length = 64\n",
    "doc_stride = 128\n",
    "do_lowercase = False\n",
    "per_gpu_train_batch_size = 12\n",
    "per_gpu_eval_batch_size = 8\n",
    "learning_rate = 3e-5\n",
    "gradient_accumulation_steps = 1\n",
    "weight_decay = 0\n",
    "max_grad_norm = 1\n",
    "num_train_epochs = 2\n",
    "warmup_steps = 0\n",
    "n_best_size = 20\n",
    "max_answer_length = 30\n",
    "seed = 42\n",
    "threads = 10\n",
    "\n",
    "\"\"\"Parameters\"\"\"\n",
    "\n",
    "\n",
    "def set_seed():\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def to_list(tensor):\n",
    "    return tensor.detach().cpu().tolist()\n",
    "\n",
    "\n",
    "def train(train_dataset, model, tokenizer, start_epoch):\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=per_gpu_train_batch_size)\n",
    "\n",
    "    t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    if os.path.isfile(os.path.join(output_dir, \"optimizer.pt\")) and os.path.isfile(\n",
    "        os.path.join(output_dir, \"scheduler.pt\")\n",
    "    ):\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(output_dir, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(output_dir, \"scheduler.pt\")))\n",
    "\n",
    "    print(\"\\nTraining:\")\n",
    "\n",
    "    global_step = 1\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    set_seed()\n",
    "\n",
    "    for epoch_idx in range(num_train_epochs):\n",
    "        if epoch_idx < start_epoch:\n",
    "            continue\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration in epoch {}\".format(epoch_idx+1))\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2],\n",
    "                \"start_positions\": batch[3],\n",
    "                \"end_positions\": batch[4],\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]\n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean()\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "        checkpoint_output_dir = os.path.join(output_dir, 'checkpoint_{}'.format(epoch_idx+1))\n",
    "        model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "        model_to_save.save_pretrained(checkpoint_output_dir)\n",
    "        tokenizer.save_pretrained(checkpoint_output_dir)\n",
    "        torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "        torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "\n",
    "def evaluate(model, tokenizer):\n",
    "    dataset, examples, features = load_and_cache_examples(tokenizer, evaluate=True, output_examples=True)\n",
    "\n",
    "    eval_sampler = SequentialSampler(dataset)\n",
    "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=per_gpu_eval_batch_size)\n",
    "\n",
    "    print(\"\\nEvaluation:\")\n",
    "\n",
    "    all_results = []\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2],\n",
    "            }\n",
    "            feature_indices = batch[3]\n",
    "            outputs = model(**inputs, return_dict=False)\n",
    "\n",
    "            for i, feature_index in enumerate(feature_indices):\n",
    "                eval_feature = features[feature_index.item()]\n",
    "                unique_id = int(eval_feature.unique_id)\n",
    "                output = [to_list(output[i]) for output in outputs]\n",
    "\n",
    "                start_logits, end_logits = output\n",
    "                result = SquadResult(unique_id, start_logits, end_logits)\n",
    "\n",
    "                all_results.append(result)\n",
    "\n",
    "    evalTime = timeit.default_timer() - start_time\n",
    "    print(\"  Evaluation done in total {} secs ({} sec per example)\".format(evalTime, evalTime / len(dataset)))\n",
    "\n",
    "    output_prediction_file = os.path.join(output_dir, \"predictions.json\")\n",
    "    output_nbest_file = os.path.join(output_dir, \"nbest_predictions.json\")\n",
    "\n",
    "    if version_2:\n",
    "        output_null_log_odds_file = os.path.join(output_dir, \"null_odds.json\")\n",
    "    else:\n",
    "        output_null_log_odds_file = None\n",
    "    \n",
    "    predictions = compute_predictions_logits(\n",
    "        examples,\n",
    "        features,\n",
    "        all_results,\n",
    "        n_best_size,\n",
    "        max_answer_length,\n",
    "        do_lowercase,\n",
    "        output_prediction_file,\n",
    "        output_nbest_file,\n",
    "        output_null_log_odds_file,\n",
    "        False,\n",
    "        version_2,\n",
    "        null_score_diff_threshold,\n",
    "        tokenizer,\n",
    "    )\n",
    "\n",
    "    if output_null_log_odds_file is not None:\n",
    "        filename = os.path.join(output_dir, 'null_odds.json')\n",
    "        null_odds = json.load(open(filename, 'rb'))\n",
    "    else:\n",
    "        null_odds = None\n",
    "    results = squad_evaluate(examples, predictions, no_answer_probs=null_odds, no_answer_probability_threshold=null_score_diff_threshold)\n",
    "    return results\n",
    "\n",
    "\n",
    "def load_and_cache_examples(tokenizer, evaluate=False, output_examples=False):\n",
    "    input_dir = os.path.join('dataset', dataset_name)\n",
    "    cached_features_file = os.path.join(\n",
    "        input_dir,\n",
    "        \"cached_{}\".format(\"test\" if evaluate else \"train\"),\n",
    "    )\n",
    "\n",
    "    if os.path.exists(cached_features_file):\n",
    "        print(\"Loading features from cached file {}\".format(cached_features_file))\n",
    "        features_and_dataset = torch.load(cached_features_file)\n",
    "        features, dataset, examples = (\n",
    "            features_and_dataset[\"features\"],\n",
    "            features_and_dataset[\"dataset\"],\n",
    "            features_and_dataset[\"examples\"],\n",
    "        )\n",
    "    else:\n",
    "        print(\"Creating features from dataset file at {}\".format(input_dir))\n",
    "        processor = SquadV2Processor() if version_2 else SquadV1Processor()\n",
    "        if evaluate:\n",
    "            examples = processor.get_dev_examples(input_dir, filename='test.json')\n",
    "        else:\n",
    "            examples = processor.get_train_examples(input_dir, filename='train.json')\n",
    "        features, dataset = squad_convert_examples_to_features(\n",
    "            examples=examples,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=max_seq_length,\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            is_training=not evaluate,\n",
    "            return_dataset=\"pt\",\n",
    "            threads=threads,\n",
    "        )\n",
    "        print(\"Saving features into cached file {}\".format(cached_features_file))\n",
    "        torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n",
    "\n",
    "    if output_examples:\n",
    "        return dataset, examples, features\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def main():\n",
    "    set_seed()\n",
    "\n",
    "    global_step = \"\"\n",
    "\n",
    "    model_file = os.path.join(output_dir,'pytorch_model.bin')\n",
    "    do_train = not os.path.exists(model_file)\n",
    "    if do_train:\n",
    "        checkpoints_dir = filter(lambda x:x.startswith('checkpoint_'), os.listdir(output_dir))\n",
    "        checkpoint = max(map(lambda x:int(x[x.find('_')+1:]), checkpoints_dir), default=0)\n",
    "        if checkpoint == 0:\n",
    "            config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, do_lower_case=do_lowercase, use_fast=False)\n",
    "            train_dataset = load_and_cache_examples(tokenizer, evaluate=False, output_examples=False)\n",
    "            model = AutoModelForQuestionAnswering.from_pretrained(model_name_or_path, config=config)\n",
    "        else:\n",
    "            checkpoint_output_dir = os.path.join(output_dir, 'checkpoint_{}'.format(checkpoint))\n",
    "            config = AutoConfig.from_pretrained(checkpoint_output_dir)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(checkpoint_output_dir, do_lower_case=do_lowercase, use_fast=False)\n",
    "            train_dataset = load_and_cache_examples(tokenizer, evaluate=False, output_examples=False)\n",
    "            model = AutoModelForQuestionAnswering.from_pretrained(checkpoint_output_dir, config=config)\n",
    "        model.to(device)\n",
    "        global_step, tr_loss = train(train_dataset, model, tokenizer, checkpoint)\n",
    "        print(\" global_step = {}, average loss = {}\".format(global_step, tr_loss))\n",
    "        print(\"Saving model checkpoint to {}\".format(output_dir))\n",
    "        model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "        model_to_save.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    result = {}\n",
    "    print(\"Evaluate the last checkpoint\")\n",
    "    config = AutoConfig.from_pretrained(output_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(output_dir, do_lower_case=do_lowercase, use_fast=False)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(output_dir, config=config)\n",
    "    model.to(device)\n",
    "    eval_result = evaluate(model, tokenizer)\n",
    "    result = dict((k, v) for k, v in eval_result.items())\n",
    "\n",
    "    print(result)\n",
    "    print(\"\\nResult:\\n\\texact_match: {}\\n\\tf1: {}\".format(result['exact'], result['f1']))\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1638645758546,
     "user": {
      "displayName": "Jamshid Mozafari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLKdl-M6ZkOmHKMDfhTduwgXYNb7xMJFVQ5D12=s64",
      "userId": "09439038486094053669"
     },
     "user_tz": -210
    },
    "id": "YNYv7QW-1D8_",
    "outputId": "9f915ecf-4a4e-4f29-c84e-ef32f7daf928"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Inference.py\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "from argparse import ArgumentParser\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "def to_list(tensor):\n",
    "    return tensor.detach().cpu().tolist()\n",
    "\n",
    "def get_qa_inputs(question, context, tokenizer):\n",
    "    return tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "\n",
    "def get_clean_text(tokens, tokenizer):\n",
    "    text = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(tokens)\n",
    "        )\n",
    "    text = text.strip()\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def prediction_probabilities(predictions):\n",
    "\n",
    "    def softmax(x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    all_scores = [pred.start_logit+pred.end_logit for pred in predictions] \n",
    "    return softmax(np.array(all_scores))\n",
    "\n",
    "def preliminary_predictions(start_logits_, end_logits_, input_ids, nbest):\n",
    "    start_logits = to_list(start_logits_)[0]\n",
    "    end_logits = to_list(end_logits_)[0]\n",
    "    tokens = to_list(input_ids)[0]\n",
    "\n",
    "    start_idx_and_logit = sorted(enumerate(start_logits), key=lambda x: x[1], reverse=True)\n",
    "    end_idx_and_logit = sorted(enumerate(end_logits), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    start_indexes = [idx for idx, logit in start_idx_and_logit[:nbest]]\n",
    "    end_indexes = [idx for idx, logit in end_idx_and_logit[:nbest]]\n",
    "\n",
    "    question_indexes = [i+1 for i, token in enumerate(tokens[1:tokens.index(102)])]\n",
    "\n",
    "    PrelimPrediction = collections.namedtuple(\n",
    "        \"PrelimPrediction\", [\"start_index\", \"end_index\", \"start_logit\", \"end_logit\"]\n",
    "    )\n",
    "    prelim_preds = []\n",
    "    for start_index in start_indexes:\n",
    "        for end_index in end_indexes:\n",
    "            if start_index in question_indexes:\n",
    "                continue\n",
    "            if end_index in question_indexes:\n",
    "                continue\n",
    "            if end_index < start_index:\n",
    "                continue\n",
    "            prelim_preds.append(\n",
    "                PrelimPrediction(\n",
    "                    start_index = start_index,\n",
    "                    end_index = end_index,\n",
    "                    start_logit = start_logits[start_index],\n",
    "                    end_logit = end_logits[end_index]\n",
    "                )\n",
    "            )\n",
    "    prelim_preds = sorted(prelim_preds, key=lambda x: (x.start_logit + x.end_logit), reverse=True)\n",
    "    return prelim_preds, (tokens, start_logits, end_logits)\n",
    "\n",
    "\n",
    "def best_predictions(prelim_preds, nbest, tok_logits, tokenizer):\n",
    "    tokens, start_logits, end_logits = tok_logits\n",
    "    BestPrediction = collections.namedtuple(\n",
    "        \"BestPrediction\", [\"text\", \"start_logit\", \"end_logit\"]\n",
    "    )\n",
    "    nbest_predictions = []\n",
    "    seen_predictions = []\n",
    "    for pred in prelim_preds:\n",
    "        if len(nbest_predictions) >= nbest: \n",
    "            break\n",
    "        if pred.start_index > 0:\n",
    "            toks = tokens[pred.start_index : pred.end_index+1]\n",
    "            text = get_clean_text(toks, tokenizer)\n",
    "\n",
    "            if text in seen_predictions:\n",
    "                continue\n",
    "\n",
    "            seen_predictions.append(text) \n",
    "\n",
    "            nbest_predictions.append(\n",
    "                BestPrediction(\n",
    "                    text=text, \n",
    "                    start_logit=pred.start_logit,\n",
    "                    end_logit=pred.end_logit\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "    nbest_predictions.append(\n",
    "        BestPrediction(\n",
    "            text=\"\", \n",
    "            start_logit=start_logits[0], \n",
    "            end_logit=end_logits[0]\n",
    "            )\n",
    "        )\n",
    "    return nbest_predictions\n",
    "\n",
    "\n",
    "def compute_score_difference(predictions):\n",
    "    score_null = predictions[-1].start_logit + predictions[-1].end_logit\n",
    "    score_non_null = predictions[0].start_logit + predictions[0].end_logit\n",
    "    return score_null - score_non_null\n",
    "\n",
    "\n",
    "def get_robust_prediction(question, context, model, tokenizer, nbest=10, null_threshold=1.0):\n",
    "    \n",
    "    inputs = get_qa_inputs(question, context, tokenizer)\n",
    "    start_logits, end_logits = model(**inputs, return_dict=False)\n",
    "\n",
    "    prelim_preds, tok_logits = preliminary_predictions(start_logits, \n",
    "                                           end_logits, \n",
    "                                           inputs['input_ids'],\n",
    "                                           nbest)\n",
    "    \n",
    "    nbest_preds = best_predictions(prelim_preds, nbest, tok_logits, tokenizer)\n",
    "\n",
    "    probabilities = prediction_probabilities(nbest_preds)\n",
    "        \n",
    "    score_difference = compute_score_difference(nbest_preds)\n",
    "\n",
    "    if score_difference > null_threshold:\n",
    "        return \"#NO_ANSWER#\", probabilities[-1]\n",
    "    else:\n",
    "        return nbest_preds[0].text, probabilities[0]\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--text', type=str, required=True)\n",
    "    parser.add_argument('--question', type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    text = args.text\n",
    "    question = args.question\n",
    "\n",
    "    output_dir = 'model'\n",
    "    do_lowercase = True\n",
    "\n",
    "    config = AutoConfig.from_pretrained(output_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(output_dir, do_lower_case=do_lowercase, use_fast=False)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(output_dir, config=config)\n",
    "\n",
    "    answer, prob = get_robust_prediction(question, text, model, tokenizer, nbest=10, null_threshold=0)\n",
    "    print('Question: {}'.format(question))\n",
    "    print('Answer: {}\\n'.format(answer))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29135,
     "status": "ok",
     "timestamp": 1638645787676,
     "user": {
      "displayName": "Jamshid Mozafari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgLKdl-M6ZkOmHKMDfhTduwgXYNb7xMJFVQ5D12=s64",
      "userId": "09439038486094053669"
     },
     "user_tz": -210
    },
    "id": "S4rbwI301Z_H",
    "outputId": "695f500f-1834-4bf9-df8c-717d83e2b304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: مسی اهل کدام کشور هست؟\n",
      "Answer: ارژانتین\n",
      "\n",
      "Question: پست مسی چیست؟\n",
      "Answer: مهاجم و بال\n",
      "\n",
      "Question: مسی فاتح چند عنوان بهترین بازیکن سال فوتبال جهان هست؟\n",
      "Answer: شش بار\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python Inference.py --question=\"مسی اهل کدام کشور هست؟\" --text=\"لیونل آندرس «لئو» مسی کوچیتینی بازیکن فوتبال اهل کشور آرژانتین است که برای باشگاه بارسلونا و تیم ملی آرژانتین در پست های مهاجم و بال بازی می‌کند. وی اولین و تنها بازیکن در تاریخ فوتبال است که شش بار فاتح عنوان بهترین بازیکن سال فوتبال جهان و شش بار برندهٔ جوایز توپ طلا و کفش طلای اروپا شده است؛ لذا بسیاری از صاحب‌نظران برجسته او را بهترین بازیکن تاریخ فوتبال می‌دانند.\"\n",
    "!python Inference.py --question=\"پست مسی چیست؟\" --text=\"لیونل آندرس «لئو» مسی کوچیتینی بازیکن فوتبال اهل کشور آرژانتین است که برای باشگاه بارسلونا و تیم ملی آرژانتین در پست های مهاجم و بال بازی می‌کند. وی اولین و تنها بازیکن در تاریخ فوتبال است که شش بار فاتح عنوان بهترین بازیکن سال فوتبال جهان و شش بار برندهٔ جوایز توپ طلا و کفش طلای اروپا شده است؛ لذا بسیاری از صاحب‌نظران برجسته او را بهترین بازیکن تاریخ فوتبال می‌دانند.\"\n",
    "!python Inference.py --question=\"مسی فاتح چند عنوان بهترین بازیکن سال فوتبال جهان هست؟\" --text=\"لیونل آندرس «لئو» مسی کوچیتینی بازیکن فوتبال اهل کشور آرژانتین است که برای باشگاه بارسلونا و تیم ملی آرژانتین در پست های مهاجم و بال بازی می‌کند. وی اولین و تنها بازیکن در تاریخ فوتبال است که شش بار فاتح عنوان بهترین بازیکن سال فوتبال جهان و شش بار برندهٔ جوایز توپ طلا و کفش طلای اروپا شده است؛ لذا بسیاری از صاحب‌نظران برجسته او را بهترین بازیکن تاریخ فوتبال می‌دانند.\"\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Main v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10b4616710d04a9dab4a084decd9eb61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2976524682494178a178f89d6b8d74e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69f8bfff9a2c443194c1a8e681e63e81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9afba3641a884e71bcf5fe7f422f7f83",
      "placeholder": "​",
      "style": "IPY_MODEL_fa8754f4faf94163ad1a72f237d1aa98",
      "value": " 270/270 [01:49&lt;00:00,  2.47it/s]"
     }
    },
    "7534515187104b7a861b67f18119768a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "965b985e83d949f08abc626ce0aa395b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9afba3641a884e71bcf5fe7f422f7f83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6e77a2835434c29af141cae971791d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10b4616710d04a9dab4a084decd9eb61",
      "placeholder": "​",
      "style": "IPY_MODEL_e56321fea37d45f6a028278d64ed2943",
      "value": "Evaluating: 100%"
     }
    },
    "c1e162c9fb3f4d4289d274ef0d31cfec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2976524682494178a178f89d6b8d74e2",
      "max": 270,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7534515187104b7a861b67f18119768a",
      "value": 270
     }
    },
    "e56321fea37d45f6a028278d64ed2943": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eeadf78980f54a858ace61f6dd666dc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a6e77a2835434c29af141cae971791d9",
       "IPY_MODEL_c1e162c9fb3f4d4289d274ef0d31cfec",
       "IPY_MODEL_69f8bfff9a2c443194c1a8e681e63e81"
      ],
      "layout": "IPY_MODEL_965b985e83d949f08abc626ce0aa395b"
     }
    },
    "fa8754f4faf94163ad1a72f237d1aa98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
